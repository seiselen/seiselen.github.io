<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=Su9cPBdaKucZGyXzr7BkXw');ul.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_3-0>li:before{content:"\0025cf  "}ul.lst-kix_list_5-7{list-style-type:none}ul.lst-kix_list_5-8{list-style-type:none}.lst-kix_list_3-1>li:before{content:"\0025cb  "}.lst-kix_list_3-2>li:before{content:"\0025a0  "}ul.lst-kix_list_5-5{list-style-type:none}ul.lst-kix_list_5-6{list-style-type:none}ul.lst-kix_list_1-3{list-style-type:none}.lst-kix_list_3-5>li:before{content:"\0025a0  "}ul.lst-kix_list_5-0{list-style-type:none}ul.lst-kix_list_1-4{list-style-type:none}ul.lst-kix_list_1-1{list-style-type:none}.lst-kix_list_3-4>li:before{content:"\0025cb  "}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_5-3{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\0025cf  "}ul.lst-kix_list_5-4{list-style-type:none}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_5-1{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_5-2{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}.lst-kix_list_3-8>li:before{content:"\0025a0  "}.lst-kix_list_3-6>li:before{content:"\0025cf  "}.lst-kix_list_3-7>li:before{content:"\0025cb  "}.lst-kix_list_5-0>li:before{content:"\0025cf  "}.lst-kix_list_4-8>li:before{content:"\0025a0  "}.lst-kix_list_5-3>li:before{content:"\0025cf  "}.lst-kix_list_4-7>li:before{content:"\0025cb  "}.lst-kix_list_5-2>li:before{content:"\0025a0  "}.lst-kix_list_5-1>li:before{content:"\0025cb  "}ul.lst-kix_list_4-8{list-style-type:none}.lst-kix_list_5-7>li:before{content:"\0025cb  "}ul.lst-kix_list_4-6{list-style-type:none}.lst-kix_list_5-6>li:before{content:"\0025cf  "}.lst-kix_list_5-8>li:before{content:"\0025a0  "}ul.lst-kix_list_4-7{list-style-type:none}ul.lst-kix_list_4-0{list-style-type:none}ul.lst-kix_list_4-1{list-style-type:none}.lst-kix_list_5-4>li:before{content:"\0025cb  "}ul.lst-kix_list_4-4{list-style-type:none}.lst-kix_list_5-5>li:before{content:"\0025a0  "}ul.lst-kix_list_4-5{list-style-type:none}ul.lst-kix_list_4-2{list-style-type:none}ul.lst-kix_list_4-3{list-style-type:none}.lst-kix_list_6-1>li:before{content:"\0025cb  "}.lst-kix_list_6-3>li:before{content:"\0025cf  "}.lst-kix_list_6-0>li:before{content:"\0025cf  "}.lst-kix_list_6-4>li:before{content:"\0025cb  "}.lst-kix_list_6-2>li:before{content:"\0025a0  "}.lst-kix_list_6-8>li:before{content:"\0025a0  "}.lst-kix_list_6-5>li:before{content:"\0025a0  "}.lst-kix_list_6-7>li:before{content:"\0025cb  "}.lst-kix_list_7-0>li:before{content:"\0025cf  "}.lst-kix_list_6-6>li:before{content:"\0025cf  "}.lst-kix_list_2-6>li:before{content:"\0025cf  "}.lst-kix_list_2-7>li:before{content:"\0025cb  "}.lst-kix_list_7-4>li:before{content:"\0025cb  "}.lst-kix_list_7-6>li:before{content:"\0025cf  "}.lst-kix_list_2-4>li:before{content:"\0025cb  "}.lst-kix_list_2-5>li:before{content:"\0025a0  "}.lst-kix_list_2-8>li:before{content:"\0025a0  "}.lst-kix_list_7-1>li:before{content:"\0025cb  "}.lst-kix_list_7-5>li:before{content:"\0025a0  "}.lst-kix_list_7-2>li:before{content:"\0025a0  "}.lst-kix_list_7-3>li:before{content:"\0025cf  "}ul.lst-kix_list_7-5{list-style-type:none}ul.lst-kix_list_7-6{list-style-type:none}ul.lst-kix_list_7-3{list-style-type:none}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_7-4{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}ul.lst-kix_list_7-7{list-style-type:none}ul.lst-kix_list_7-8{list-style-type:none}ul.lst-kix_list_3-1{list-style-type:none}ul.lst-kix_list_3-2{list-style-type:none}.lst-kix_list_7-8>li:before{content:"\0025a0  "}ul.lst-kix_list_3-0{list-style-type:none}ul.lst-kix_list_7-1{list-style-type:none}ul.lst-kix_list_3-5{list-style-type:none}ul.lst-kix_list_7-2{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_7-0{list-style-type:none}.lst-kix_list_7-7>li:before{content:"\0025cb  "}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_4-0>li:before{content:"\0025cf  "}.lst-kix_list_4-1>li:before{content:"\0025cb  "}.lst-kix_list_4-4>li:before{content:"\0025cb  "}.lst-kix_list_4-3>li:before{content:"\0025cf  "}.lst-kix_list_4-5>li:before{content:"\0025a0  "}.lst-kix_list_4-2>li:before{content:"\0025a0  "}.lst-kix_list_4-6>li:before{content:"\0025cf  "}ul.lst-kix_list_6-6{list-style-type:none}ul.lst-kix_list_6-7{list-style-type:none}ul.lst-kix_list_6-4{list-style-type:none}ul.lst-kix_list_2-8{list-style-type:none}ul.lst-kix_list_6-5{list-style-type:none}ul.lst-kix_list_6-8{list-style-type:none}ul.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_1-0>li:before{content:"\0025cf  "}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}ul.lst-kix_list_6-2{list-style-type:none}ul.lst-kix_list_2-6{list-style-type:none}ul.lst-kix_list_6-3{list-style-type:none}.lst-kix_list_1-1>li:before{content:"\0025cb  "}.lst-kix_list_1-2>li:before{content:"\0025a0  "}ul.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_6-0{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_6-1{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_1-3>li:before{content:"\0025cf  "}.lst-kix_list_1-4>li:before{content:"\0025cb  "}.lst-kix_list_1-7>li:before{content:"\0025cb  "}.lst-kix_list_1-5>li:before{content:"\0025a0  "}.lst-kix_list_1-6>li:before{content:"\0025cf  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_list_2-0>li:before{content:"\0025cf  "}.lst-kix_list_2-1>li:before{content:"\0025cb  "}.lst-kix_list_1-8>li:before{content:"\0025a0  "}.lst-kix_list_2-2>li:before{content:"\0025a0  "}.lst-kix_list_2-3>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left;height:11pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c5{padding-top:16pt;padding-bottom:4pt;line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c18{padding-top:14pt;padding-bottom:6pt;line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c15{padding-top:18pt;padding-bottom:6pt;line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c14{font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c13{color:#434343;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c10{color:#000000;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c19{text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c12{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c17{font-weight:400;font-family:"Arimo"}.c6{padding:0;margin:0}.c7{background-color:#ffffff;color:#222222}.c11{margin-left:72pt;padding-left:0pt}.c20{color:#274e13;font-style:italic}.c16{border:1px solid black;margin:5px}.c9{margin-left:36pt;padding-left:0pt}.c21{margin-left:36pt}.c3{font-weight:700}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-weight:700;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c12"><div><p class="c2"><span class="c0"></span></p></div><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><h2 class="c15" id="h.gjdgxs"><span class="c10 c3">Research Mission Statement</span></h2><p class="c2"><span class="c14 c7"></span></p><p class="c4"><span class="c7">Empower creators and explorers of simulated worlds from the &#39;invisible walls and empty spaces&#39; which hide the limitations that a finite team of humans face in terms of time and effort towards crafting massive, highly detailed environments and the entities which live within them. Empower simulated worlds from the &#39;state of non-dynamic stasis&#39; which limits their ability and that of all characters, events, and environments therein to evolve beyond some discrete completion of pre-scripted events. Determine the computational limits of &lsquo;min world input - max world output&rsquo;.</span></p><h2 class="c15" id="h.1fob9te"><span class="c10 c3">Key Idea: &lsquo;Principle of Physical Stability and Contextual Consistency&rsquo;</span></h2><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">Blurb:</span><span class="c0">&nbsp;A key idea that I think is not novel but also non-existent in terms of proper definition is what I call the &lsquo;Principle of Physical Stability and Contextual Consistency&rsquo;. There is an intended sense of cohesion between the two ideas, as the complement of each encompasses the means towards effecting convergence towards optimally realistic procedurally generated simulation environments WRT both human explorers and most importantly: the native inhabitants therein. Basically, this principle asserts that optimal procedurally generated environments must keep and enforce the &lsquo;illusion of reality&rsquo; within virtual reality environments as much as possible.</span></p><p class="c2"><span class="c0"></span></p><ul class="c6 lst-kix_list_5-0 start"><li class="c4 c9 li-bullet-0"><span class="c3">Physical Stability </span><span>demands that the generated world is stable with respect to the laws which govern it; similar to how the laws of gravity, thermodynamics, etc. must always hold in order for our universe to be stable to exist within. WRT simulation worlds, this could encompass things such as: </span></li></ul><ul class="c6 lst-kix_list_5-1 start"><li class="c4 c11 li-bullet-0"><span>Mesh </span><span class="c7">colliders correct with geometry and collision mechanics behave correctly</span></li><li class="c4 c11 li-bullet-0"><span class="c7">No holes exist for terrain meshes nor holes and/or z-fighting for any surface</span></li><li class="c4 c11 li-bullet-0"><span class="c7">No static objects (i.e. trees) are &lsquo;floating in the air&rsquo; nor ever become dynamic</span></li><li class="c4 c11 li-bullet-0"><span class="c7">No physically dynamic objects &lsquo;freeze in place&rsquo; as static objects</span></li><li class="c4 c11 li-bullet-0"><span class="c7">NPCs able to navigate throughout world with no unrealistic pathing effects</span></li><li class="c4 c11 li-bullet-0"><span class="c7">NPCs travel correctly and seamlessly between sub-scenes/environments.</span></li></ul><p class="c2"><span class="c7 c3 c19"></span></p><ul class="c6 lst-kix_list_2-0 start"><li class="c4 c9 li-bullet-0"><span class="c7 c3">Contextual Consistency </span><span class="c7">demands that all content in all spaces within the generated world is consistent with all aspects of their respective context. For example: an accurate simulation of Paris, France in the 1960&rsquo;s should not have NYPD police cars from the 2010&rsquo;s patrolling the streets, nor a simulation of Middle Earth in Lord Of The Rings have Boeing 747 Jumbo Jets flying over the skies of Gondor. Further, this is a spatio-temporal assertion iff the simulation plays out a progression of events WRT some lore/storyline. Using the same Lord of the Rings simulation: assume the current timeframe is before Gandalf discovers that his fellow wizard Saruman has secretly turned against the fellowship and has aligned with the dark lord Sauron. One should be able to walk up to Gandalf&rsquo;s character, tell him that they think Saruman has become evil, and be told by Gandalf that while Saruman has been quiet-of-late, there&rsquo;s no evidence to prove this claim. </span><span>WRT simulation worlds, this could encompass things such as: </span></li></ul><ul class="c6 lst-kix_list_2-1 start"><li class="c4 c11 li-bullet-0"><span class="c14 c7">Terrain, props, architecture, buildings, NPCs consistent with surrounding area. </span></li><li class="c4 c11 li-bullet-0"><span class="c14 c7">No patch of snow covered tundra in a tropical forest or vice versa.</span></li><li class="c4 c11 li-bullet-0"><span class="c14 c7">No Borg in a DooM simulation, nor Cyberdemons in a Star Trek simulation</span></li></ul><p class="c2"><span class="c14 c7"></span></p><p class="c4"><span>To physically stable: this means things like no sudden major gaps of defined world space (i.e. forest turns into giant flat patch of land or worse - empty space as to see through the world), and no characters falling through floors or buildings floating in the sky (unintentionally, that is). To contextually consistent: this means things like no police cars, cell phones, modern clothing, nor 757 jets flying over what should be the classic fantasy setting; as well as space-time consistency which requires some more examples. For starters: if the world &ldquo;starts&rdquo; right before it&rsquo;s revealed that the wizard Saruman has turned against the fellowship and aligned with the Dark Lord Sauron, the player should be able to ask the Wizard Gandalf about Saruman and be told something.</span></p><h2 class="c15" id="h.3znysh7"><span class="c10 c3">Key Concept: &lsquo;Multi-Layered Environment Generation&rsquo;</span></h2><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">Blurb: </span><span class="c0">Another concept whose methods and implementation cases I&rsquo;ve studied and found to merit further discussion is what I call &lsquo;Multi-Layered Environment Generation&rsquo;. One of the best way to explain this concept is to describe a general algorithm for what an entire process whose &lsquo;layers&rsquo; author a completely detailed environment could entail, which is done as follows:</span></p><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">Algorithm: </span><span>Given input context [Context] as a multimedia database describing/defining some simulation environment, generate a Physically Stable, Contextually Consistent output with layering order: </span><span class="c3">[Context &gt; Terrain &gt; Roads &gt; Blocks &gt; Lots &gt; Buildings]</span><span class="c0">&nbsp;as such:</span></p><ul class="c6 lst-kix_list_6-0 start"><li class="c4 c9 li-bullet-0"><span class="c3">[Terrain] | [Context]: </span><span>Generate terrain given the input context, including terrain erosion and preparatory steps for &lsquo;fitting&rsquo; contextual content thereupon.</span></li><li class="c4 c9 li-bullet-0"><span class="c3">[Roads] | [Terrain] &amp; [Context]: </span><span>From the generated terrain and input context: Generate roads and networks thereof. This could include any type of navigable path from natural trails to railroads and interstate highways. </span></li><li class="c4 c9 li-bullet-0"><span class="c3">[Blocks] | [Roads] &amp; [Context]: </span><span>From the generated roads and input context: Generate blocks adjacent to the roads upon which structures could be placed (e.g. s.t. blocks are typically generated at the intersections and edges of roads).</span></li><li class="c4 c9 li-bullet-0"><span class="c3">[Lots] | [Blocks] &amp; [Context]:</span><span>&nbsp;Partition the blocks into individual lots e.g. on which buldings could be placed. </span></li><li class="c4 c9 li-bullet-0"><span class="c3">[Buildings] | [Lots] &amp; [Context]: </span><span>Place buildings (and other structures, too) upon the lots. This step could imply an &lsquo;upwards edit&rsquo; s.t. lots are merged to fit larger buildings.</span></li></ul><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">Questions: </span></p><ul class="c6 lst-kix_list_1-0 start"><li class="c4 c9 li-bullet-0"><span class="c3">Inter-layer Nature Of [Props]:</span><span>&nbsp;There are props corresponding to each layer, such as: traffic controls and bridges/tunnels for roads; trees for terrain, lots, and buildings; etc. There is a clear interrelationship, ergo how would this work? One case for example&hellip;</span></li><li class="c4 c9 li-bullet-0"><span class="c3">&lsquo;Upwards Edit&rsquo;: </span><span>When a lower-level generation process affects an upper-level one. The example above discusses the idea of lots being merged in the building generator step in order to increase possible building footprints; and another example across multiple layers is a infrastructure </span><span>&lsquo;shepherd&rsquo; </span><sup><a href="#cmnt1" id="cmnt_ref1">[a]</a></sup><sup><a href="#cmnt2" id="cmnt_ref2">[b]</a></sup><span>system which identifies the need to expand a 2-lane road into an 8-lane highway in especially hilly terrain: thus needing terrain editing.</span></li></ul><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c1">Precedent and Ideas:</span></p><ul class="c6 lst-kix_list_4-0 start"><li class="c4 c9 li-bullet-0"><span class="c7">Extends the current work done on composite environment generation, most of which were the city generator papers of which have implemented a 1-3 layer process (but I think nobody has gone all the way with a full 5-Layer Generator yet).</span></li></ul><p class="c2 c21"><span class="c14 c7"></span></p><ul class="c6 lst-kix_list_4-0"><li class="c4 c9 li-bullet-0"><span class="c7">Procedural Generation of high-detail 3D environments utilizing ML and a collection of other techniques to define and implement a &#39;Converged Workflow&#39; which encompasses all stages of the greater process.</span></li><li class="c4 c9 li-bullet-0"><span class="c7">A specific topic could then be an analysis for how to combine all &#39;macro&#39; and &#39;micro&#39; components / aspects of procedurally generated environments into a single theoretical and (hopefully) applied model.</span></li></ul><p class="c2"><span class="c14 c7"></span></p><h2 class="c15" id="h.2et92p0"><span class="c10 c3">Key Concept: &lsquo;Informed/Intelligent Filling-In-Of-Blanks&rsquo;</span></h2><p class="c2"><span class="c14 c7"></span></p><ul class="c6 lst-kix_list_3-0 start"><li class="c4 c9 li-bullet-0"><span class="c7">Mention TerrainGAN parallel</span></li><li class="c4 c9 li-bullet-0"><span class="c7">Basically: the AI can &#39;fill in large patches&#39; between manually designed areas which are consistent between such areas. For example: </span></li><li class="c4 c9 li-bullet-0"><span class="c7">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&Oslash; Input: </span></li><li class="c4 c9 li-bullet-0"><span class="c7">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#9675; Chunks of manually-designed environment composing downtown Raccoon City, the Spencer Mansion, and several other small defined areas partitioned by gaps which need to be filled in;</span></li><li class="c4 c9 li-bullet-0"><span class="c7">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#9675; General definitions for what should go between (e.g. this sector should be suburbs, that sector is a part of the Arklay Mountains between Spencer Mansion and Training Facility);</span></li><li class="c4 c9 li-bullet-0"><span class="c7">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#9675; Resource/Content definitions for architecture styles / props / other reference multimedia</span></li><li class="c4 c9 li-bullet-0"><span class="c7">Output: &#39;Filled-In&#39; gaps consistent with the adjacent defined areas and input reference/resource material. For example, the remainder of the Arklay Mountains, the exurbs/suburbs of Racoon City, the train tracks and infrastructure leaving the city, etc. such that each seamlessly converge with the defined sub-sectors of the greater environment.</span></li></ul><p class="c2"><span class="c14 c7"></span></p><p class="c2"><span class="c14 c7"></span></p><p class="c2"><span class="c14 c7"></span></p><p class="c2"><span class="c7 c14"></span></p><h2 class="c15" id="h.tyjcwt"><span class="c10 c3">Concept: LOA (Level Of Authoring)</span></h2><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">Overview: </span><span class="c0">LOA is my attempt to define and speak about the &lsquo;guidance&rsquo; by which PCG generates environments, especially pertaining to human involvement (i.e. &lsquo;authoring&rsquo;). A technique could use one or more of these. The current schema is:</span></p><p class="c2"><span class="c0"></span></p><ul class="c6 lst-kix_list_7-0 start"><li class="c4 c9 li-bullet-0"><span class="c3">Sketch-Based: </span><span class="c0">Human provides a rough sketch of what some environment should look like. Roughly speaking: it includes drawing shapes to represent the basic geometry of some feature, and glyphs to describe what the feature represents. For example, splines and spline polygons to indicate terrain alongside the desired feature (e.g. [R] = river, [M] = mountain, [L] = lake); boxes, circles, or some polygon to represent settlements alongside the desired type (e.g. [V] = village, [T] = town, [C] = city); as well as paths to indicate desired connections between settlements. This would be the method most suitable for users who have a clear idea on what they want an environment to look like, both spatially and contextually.</span></li><li class="c4 c9 li-bullet-0"><span class="c3">Description-Based: </span><span>Basically a &lsquo;List&rsquo; of what the user wants in the environment, with the remaining details left to the generator. For Example: </span><span class="c20">&ldquo;Generate a mountainous environment with a river going through it, three towns located along the river spaced equidistant from one another, a few mines into the mountain, and a castle. Connect the towns, mines, and castle all with each other and preferably on mostly flat paths that align with the river. Then place random inns, farms, and small villages along the roads but not too close to each other and the other settlements. Lastly, place random bandit camps throughout the environment but nowhere within &frac12; mile of a settlement&rdquo;</span><span class="c0">. This method would be suitable for users who have an idea for what should be within an environment, but don&rsquo;t have any specification for how the environment should be arranged.</span></li><li class="c4 c9 li-bullet-0"><span class="c3">Similarity-Based: </span><span class="c0">The most ML-suited method. The user does not have an idea for what exactly should be in an environment, but can provide the system with some examples. For example: &ldquo;Create this particular region to have a giant city and surrounding area in a manner: similar in appearance to Elder Scrolls&rsquo; Imperial City crossed with London UK, in a desert environment similar to Arizona, and set in the United States in the 1980&rsquo;s&rdquo;</span></li><li class="c4 c9 li-bullet-0"><span class="c3">FITB-Based (Fill-In-The-Blanks): </span><span>Similar to the Similarity-Based method, but generates an environment based largely on whatever surrounding environments have already been generated. Demonstrated with TerrainGAN. Basically: &ldquo;We have manually designed the major city, towns, and some other parts of some region, but most of the remaining area remains blank (undefined). Fill the gaps in with a mountain over here, valleys over here, a lake somewhere in here, and otherwise as corresponding with the surrounding area.<br><br></span><span class="c3">Side Note: </span><span class="c0">This was a feature I found most interesting/promising with TerrainGAN, and implementing it alongside the &#39;future work&#39; of additional terrain types (including urban areas) was a major motive of which Proto-Moonshot in Carlos ML course was initially planning to explore (before being scaled down)<br></span></li></ul><p class="c2"><span class="c0"></span></p><h2 class="c15" id="h.3dy6vkm"><span class="c3 c10">Idea: Use of &lsquo;Context Bitstrings&rsquo;</span></h2><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c3">Overview: </span><span class="c0">Alex Koltz reminded me about the technique of using bitstring hash codes as a KR encoding for newly generated (i.e. &lsquo;On Init&rsquo;) environments as to effect some degree of persistence (i.e. stays the same post-generation ergo does not [fully] regenerate).</span></p><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">Idea: </span><span class="c17">The perspective by which he reintroduced the idea was for the proposition of focusing my work on the [Building Interior] | [Building]&and;[Context] aspect; as for an interior, once generated, to persist thereafter. An immediate extrapolation is to offer persistence such that the static geometry perists unchanged alongside a contextual definition for the interiors&rsquo; props and NPCs which is subject to modification (e.g. swapping out NPCs and loot items over time as per E.S. Oblivion). </span></p><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">1/5/2020 Update: </span><span class="c0">The main if not only purpose of this would be to compress the &lsquo;contextual definition&rsquo; of the generated content; both in terms of its static definition (i.e. &ldquo;this is a grocery store&rdquo;) and dynamic/unique definition (i.e. &ldquo;it was looted&rdquo;, &ldquo;it was overrun with zombies at some point&rdquo;).</span></p><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">Example: </span><span class="c0">For example of a contextual definition, an encoding could represent the directive: &ldquo;This interior space was generated to be a floor of hospital rooms and associated stuff following a zombie apocalypse&rdquo; as for the system to &lsquo;intelligently&rsquo; scatter loot and other items throughout the area corresponding to hospital rooms, labs, custodial equipment, etc. while placing zombie NPCs such that most are in the form of zombie nurses, orderlies, doctors, paramedics, patients, and other types that would be expected in a hospital floor.</span></p><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">Issues / Challenges / Questions: </span><span class="c0">Mostly involving how such compression and reducibility could be done and to what degree. Clearly the geometry can less easily be encompassed in a bitstring. However, there might be merit in perhaps implementing a [geometry,context] data-pair wherein the geometry half is unavoidably a mesh generated @Init, but the context supports the @Init generation thereof while providing a more feasible encoding for how and what could be placed into the environment. So we fully persist the geometric &ldquo;walls floors ceilings&rdquo; definition, but contextually persist what gets placed within. And given the polygon costs of even &lsquo;game-ready&rsquo; props, this arrangement actually saves much of the required model storage needed. Furthermore, we could actually persist prop placement &hellip; via only keeping a [Transform,ObjectID] entry in some data structure associated with the environment!</span></p><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">Thus - The Concept: </span><span class="c0">We will allow you to explore all 60 floors of an office building or however many you have the time and patience to explore, even though the developers only designed the building&rsquo;s exterior (if even that - but this is a different PCG method!) We will also guarantee that, once generated, the geometry of the 48th floor of this building will stay the same the next time you visit it, as well as some of the objects placed therein. Correspondingly: we need to procedurally generate the geometry for each floor, and the client (you) must store the geometry created and consequently incur the data costs thereof; but you could choose whether of not the prop items placed within are also saved as [Transform,ObjectID] pairs, and we encode the context of what this floor represents in a bitstring versus some larger data representation.</span></p><p class="c2"><span class="c0"></span></p><h2 class="c15" id="h.1t3h5sf"><span class="c10 c3">Idea: GUI&rsquo;s for Multi-Level World Generators</span></h2><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">A &ldquo;Real Dream&rdquo; for PCG via a major &lsquo;detailed 3D world-creator&rsquo; benchmark </span><span class="c0">would be the implementation of a system with a UI similar to the Tiberian Sun map editor that would allow for the creation of a high quality 3D world (including interior spaces, props, NPC spawners, etc.) as to be used for some open world RPG/exploration game (e.g. &lsquo;Fallout/Elder Scrolls clone&rsquo;). Similarly, a relatively limited set of parameters would be offered to the user via the UI, such as style of terrain, number of towns, forests, etc. Actually - this would also be analogous to, if not explicit extension of map editors for &lsquo;Civilization/Sim&rsquo; style games e.g. Sim City 4 and Civilization IV. The realization (i.e. output environment) of this dream would be on the level of a &ldquo;open world DLC/expansion&rdquo; as produced by a game studio and/or modding collaboration; as for the user to easily realize the phrase &ldquo;Create for me a mid-sized city and surrounding areas, along a river with a mountain to the north, in the style and context of this context database, for the post-apocalypse &lsquo;Fallout 3 Clone&rsquo; I bought/developed as implemented within some game engine (i.e. platform-agnostic).</span></p><p class="c2"><span class="c0"></span></p><p class="c4"><span class="c3">I think this dream could be realized</span><span class="c0">&nbsp;by an implemented of the &lsquo;Eiselen Complete World-Gen Model&rsquo; whose layers unify the Kelly/McCabe &lsquo;CityGen&rsquo; system (covering Terrain---Buildings), my proposed &lsquo;Blackreach-Erebor&rsquo; system (covering [Interiors/Props] | [Terrain---Buildings]), and the other parts needed to complete the World-Gen idea. Of course, the techniques for all of these are many, and especially feature methods introduced in the various &lsquo;Wonka Papers&rsquo;.</span></p><p class="c2"><span class="c0"></span></p><h2 class="c18" id="h.4d34og8"><span class="c10 c3">Key Concept: Distinguishing PCG Methods</span></h2><h3 class="c5" id="h.2s8eyo1"><span>Overview and Introduction</span></h3><p class="c4"><span class="c0">It&rsquo;s important to distinguish between methods which utilize Machine Learning (hereafter ML) versus those that do not, as I&rsquo;ve discovered that the same reason why it&rsquo;s valid to call ML-based methods &lsquo;Procedural Generation&rsquo; are the same as why it&rsquo;s invalie. Basically, to use an example from AI: It would be like not distinguishing between reflex, utility, and learning agents. Each are composed of algorithms and data structures, of which might each have some similarities, and of which each could accomplish the same task on the same data: but they&rsquo;re otherwise relatively (and WRT AI: fundamentally) different. While the textbook &ldquo;Procedural Content Generation in Games&rdquo; provides a good contrast, I will offer my own &lsquo;Quick-And-Dirty&rsquo; definitions as follows; followed by the concept of &lsquo;Generator-Empowered Workflows&rsquo; which can (and does) bridge these types together.</span></p><h3 class="c5" id="h.17dp8vu"><span class="c13 c3">Algorithmic-Based Content Generation</span></h3><p class="c4"><span>Basically - any content generation method that does NOT utilize ML, nor a trained network and/or generator, nor encompasses a dynamic agent. Uses &lsquo;fixed&rsquo; algorithms and pre-defined / fully manually-tuned rulesets for content generation, and typically encompass offline algorithms.</span></p><h3 class="c5" id="h.3rdcrjn"><span class="c13 c3">Agent-Based Content Generation</span></h3><p class="c4"><span class="c0">Does NOT utilize ML, nor a trained network and/or generator, but does encompass some kind of dynamic agent(s) and/or groups thereof to actively generate content as &lsquo;creatures physically interacting with it&rsquo; within a simulation. Can utilize both offline and online algorithms (i.e. terrain/vegetation erosion agents)</span></p><h3 class="c5" id="h.26in1rg"><span class="c13 c3">ML-Based Content Generation</span></h3><p class="c4"><span class="c0">Uses ML via a trained network and/or generator, which in turn utilize manually-tunable high-level rulesets (i.e. hyperparameters) on training examples (via offline and/or online algorithms) to tune low-level weights (i.e. network) as to inform some model representation to perform the requested task.</span></p><h3 class="c5" id="h.lnxbz9"><span class="c3 c13">Generator-Empowered Workflows</span></h3><p class="c4"><span class="c0">Generator-Empowered workflows encompasse combining multiple types of the above methods towards a greater implementation, for which there is a great example in the &lsquo;Terrain GAN&rsquo; paper and its implementation. The system utilized ML-based methods to generate the base terrain, and then utilized algorithmic-based method to effect erosion upon the generated basic terrains as a post-processing step. As a side note: careful modularity and common interfacing between method types and their interconnections across a &lsquo;generator pipeline&rsquo; could accommodate the ability of &lsquo;switching and swapping&rsquo; between multiple types of multiple kinds of module &lsquo;tools&rsquo;; in a manner similar to how the A* search algorithm could be programmed to swap between use of Euclidean xor Manhattan distance as the distance heuristic for its informed goal data.</span></p><div class="c16"><p class="c8"><a href="#cmnt_ref1" id="cmnt1">[a]</a><span class="c0">Actually - this is the answer to these questions. &#39;Inter-Layer&#39; Authoring systems which &#39;guide&#39; world construction. Why not think of city generation e.g. as a process in which generation oscillates up and down the layers throughout the authoring process?</span></p></div><div class="c16"><p class="c8"><a href="#cmnt_ref2" id="cmnt2">[b]</a><span class="c0">Also - I believe the Wonka Shape Grammars research did implement upwards editing with their building and maybe even tree generators</span></p></div></body></html>